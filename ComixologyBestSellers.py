#!/usr/bin/python
#coding=utf-8
!pip install -U fake-useragent
!pip install -U func_timeout
from bs4 import BeautifulSoup
from lxml import html as h
from fake_useragent import UserAgent
from google.colab import drive
from math import ceil
from posixpath import normpath
from urllib.parse import urlencode, urljoin, urlparse, urlparse, urlunparse
from datetime import date, datetime, timedelta
import pandas as pd
import csv, func_timeout, html, os.path, pickle, re, requests, string, time
 
#drive.mount('/content/drive')
print(str(UserAgent().random))
 
def getHTMLText(url, code = 'utf-8'):
    Headers = {'User-Agent':str(UserAgent().random)}
    r = requests.get(url, headers = Headers, timeout = 30)
    r.raise_for_status()
    r.encoding = code
    return r
 
def getHTMLText_with_retry(url, code = 'utf-8', retry = 10):
    for i in range(retry):
        try:
            request_text = getHTMLText(url, code='utf-8')
            return request_text
        except Exception as e:
            print(f'ç½‘é¡µè®¿é—®å¤±è´¥: {e}')
        if i > 5:
            time.sleep(10)
    print(f'Load {url} failed 10 times')
    return ''
 
def cleanLink(link):
    link = re.sub('\?ref=.*', '', link)#remove ?ref=
    attrs = re.compile(r'lang=\d+|cu=\d+')# clean cu = 0, avoid different link for same page
    link = attrs.sub('',link)
    while link.endswith('?') or link.endswith('&'):
        link = link[:-1]
    return link
 
def newUrl(base, url):
    url1 = urljoin(base, url)
    arr = urlparse(url1)
    path = normpath(arr[2])
    return urlunparse((arr.scheme, arr.netloc, path, arr.params, arr.query, arr.fragment))
 
def embNumbers(s):
    re_digits = re.compile(r'(\d+)')
    pieces = re_digits.split(s)
    pieces[1::2] = map(int,pieces[1::2])    
    return pieces
 
def sortList(alist):#sort_strings_with_embNumbers
    aux = [(embNumbers(s),s) for s in alist]
    aux.sort()
    return [s for __,s in aux]
#æ²¡ææ‡‚ï¼ŒæŒ‰https://www.cnblogs.com/ajianbeyourself/p/5395653.htmlï¼Œæ‰¾æœºä¼šç”¨keyæ›¿æ¢æ‰DSUæ’åº
 
def getListPlus(url, urlLists = list()):
    flatten = lambda x: [y for l in x for y in flatten(l)] if type(x) is list else [x]
    pageHtml = getHTMLText_with_retry(url, 'uft-8')
    dom_tree = h.fromstring(pageHtml.content)
    a = dom_tree.xpath("//a[contains(@class,'pager-link')]")#æŸ¥æ‰¾æ‰€æœ‰é¡µé¢é“¾æ¥
    try:
        listTitle = dom_tree.xpath("//h3[contains(@class,'list-title')]/text()")[0]
        try:
            listSubitle = dom_tree.xpath("//h4[contains(@class,'list-subtitle')]/text()")[0]
        except:
            listSubitle = ''
        pageTitle(listTitle, listSubitle)
    except:
        pass
    if len(a) == 0: # å¦‚æœåªæœ‰ä¸€é¡µ
        if url not in urlLists:
            urlLists.append(cleanLink(url))
    else:
        pages = []
        #urlLists.append(cleanLink(url))
        # Get all existing page link
        pages = [a[aElement].attrib['href'] for aElement in range(len(a))]
        # For total page > 5, some page is hidden
        lastPages = [a[aElement].attrib['href'] for aElement in range(len(a)) if a[aElement].text == 'Last']# links[index]è¿”å›çš„æ˜¯ä¸€ä¸ªå­—å…¸
        # generate page for hidden page
        for lastPage in lastPages:
            result = re.match('(.*_pg=)(\d+)', lastPage)
            basePath = result.group(1)
            pageNum = int(result.group(2))
            #print(f'path {basePath} {pageNum}')
            pages.extend([f'{basePath}{i+1}' for i in range(pageNum)])
        pages = [cleanLink(newUrl(url, link)) for link in pages]#æ‹¼æ¥ç½‘å€
        bundle = re.compile(r'/bundle/')#ç›®å‰æ— æ³•è§£å†³çˆ¬å–å¥—è£…é¡µï¼Œå…ˆå‰”é™¤
        pages = [link for link in pages if not bundle.search(link)]
        pages = sortList(flatten(list(set(pages))))
        urlLists.extend(pages)
    return urlLists

def cleanTitle(title):
    if '(of' in title:
        name = re.search('(.*)#(\d+) \(of (\d+)', title)
        if len(name.group(2)) < 2:
            issue = name.group(2).zfill(2)
        else:
            issue = name.group(2)
        if len(name.group(3)) < 2:
            total = name.group(3).zfill(2)
        else:
            total = name.group(3)
        title = f'{name.group(1)}{issue} (of {total})'
    if '#' in title:
        #((\d{1,3}(,\d{3})*(\.\d+)?|\d+(\.\d+)?)
        try:
            name = re.search('(.*)#(\d+)(\..*)?', title)
            if len(name.group(2)) < 3:
                issue = name.group(2).zfill(3)
                if name.group(3) != None and name.group(3) != '':
                    title = f'{name.group(1)}{issue}{name.group(3)}'
                else:
                    title = f'{name.group(1)}{issue}'
            else:
                title = title.replace('#','')
        except:
            title = title.replace('#','')
    if 'Vol. ' in title:
        title = title.replace('Vol. ','Vol.')
    title = re.sub('( \(20.{2,3}\)| \(19.{2,3}\))','',title)
    return title

def getInfo(urlLists):
    count = 0
    for url in urlLists:
        page = getHTMLText_with_retry(url)
        dom_tree = h.fromstring(page.content)
        for item in dom_tree.xpath("//li[@class='content-item']"):
            try:
                count += 1
                title = html.unescape(item.xpath(".//img[@class='content-img']/@title")[0]).strip().replace('   ',' ').replace('  ',' ')
                title = cleanTitle(title)
                if count == 1:
                    print(f'ğŸ¥‡No.{count}	{title}')
                elif count == 2:
                    print(f'ğŸ¥ˆNo.{count}	{title}')
                elif count == 3:
                    print(f'ğŸ¥‰No.{count}	{title}')
                else:
                    print(f'No.{count}	{title}')
                outputPath = f"/content/drive/My Drive/Best Sellers/{time.strftime('%Y%m%d')}Best Sellers.csv"
                if not os.path.exists(os.path.dirname(outputPath)):
                    os.makedirs(os.path.dirname(outputPath))
                with open(outputPath, 'at', encoding = 'utf-8', newline = '') as csvfile:
                    writer = csv.writer(csvfile)
                    writer.writerow([count, title, time.strftime('%Y/%m/%d %-H:%-M:%-S')])
            except:# Exception as e:
                #print(e)
                retryList = []
                count = count - currentCount
                retryList.append(url)
                getInfo(retryList)
                time.sleep(1)
                continue
        print(f'''â€¦â€¦
Comixology, {time.strftime('%Y/%m/%d')}
ğŸ“… Week of {(datetime.now() - timedelta(days = datetime.now().isoweekday()-3, weeks = 1)).strftime('%m/%d')}~{(datetime.now() - timedelta(days = datetime.now().isoweekday()-3)).strftime('%m/%d')}
''')
 
if __name__ == "__main__":
    flatten = lambda x: [y for l in x for y in flatten(l)] if type(x) is list else [x]
    start_time = time.time()
    print(f'''----------------------------------------
ä»Šå¤©æ˜¯{time.strftime('%Y/%m/%d ç¬¬%Wå‘¨ æ˜ŸæœŸ%w')}
Processing Best Sellers https://www.comixology.com/comics-best-sellers''')
    Links = []
    Links.extend(getListPlus('https://www.comixology.com/comics-best-sellers'))
    Links = sortList(flatten(list(set(Links))))#æŒ‰é¡µç æ’åº
    print(f'''Find {len(Links)} page(s)
{(datetime.now() - timedelta(days = datetime.now().isoweekday()-3)).strftime('%Y/%m/%d')} ~{time.strftime('%m/%d')}''')
    getInfo(Links)
    print(f'''
{time.strftime('%Y/%m/%d %-H:%-M')}
Finish Time: {datetime.fromtimestamp(time.time()-start_time).strftime('%-H:%-M:%-S.%f')}''')#åŠ çŸ­æ¨ªçº¿-çœç•¥å¤šä½™å‰ç½®0
    exit()
